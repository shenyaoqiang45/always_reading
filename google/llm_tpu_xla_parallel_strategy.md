# 🚀 LLM/TPU/XLA 并行策略

大规模语言模型分布式训练完全指南

深度解析数据并行、张量并行、流水线并行及其混合策略

最后更新：2025年11月

📋 概述

为什么需要并行策略？

现代LLM的参数量达到千亿到万亿级别，单机无法容纳模型和数据。分布式训练必须在多台设备（TPU/GPU）上协同执行。关键问题是：

  * **如何分割模型？** ——张量并行 vs 流水线并行
  * **如何分布数据？** ——数据并行与样本分片
  * **如何优化通信？** ——集合通信与梯度同步
  * **如何选择策略？** ——根据模型规模和硬件拓扑

✓ Google TPU + XLA 优势

Google提供的TPU集群与XLA编译器为大规模分布式训练优化：

  * **硬件专优** ：TPU间通过高速互连（ICI）通讯，带宽高达600GB/s
  * **编译器自动化** ：XLA的SPMD编译传递自动生成分布式代码
  * **端到端优化** ：从高级代码到硬件指令的全链路优化

🎯 并行策略基础

1\. 数据并行（Data Parallelism）

定义与原理

将数据样本分散到不同设备，每个设备执行相同的模型计算，然后同步梯度。

**执行流程：**

设备1：[样本1-100] → 模型 → 梯度 ↓  
设备2：[样本101-200] → 模型 → 梯度 ↓ AllReduce  
设备3：[样本201-300] → 模型 → 梯度 ↓  
同步梯度 → 参数更新 

**✓ 优点**

  * 实现简单，几乎所有框架都支持
  * 通信量相对较小（仅同步梯度）
  * 无需修改模型代码
  * 扩展性好（线性扩展到通信成本）

**✗ 缺点**

  * 无法解决单个样本过大的问题
  * 无法扩展到超大模型（参数量超过单机显存）
  * 通信成本随梯度量增加（GB级别）
  * 每个设备需要完整副本

2\. 张量并行（Tensor Parallelism）

定义与原理

将模型的权重矩阵分割到不同设备，单个样本跨多个设备进行计算。

**矩阵乘法分割例子：**

输入 x [batch, seq_len, hidden_dim]  
权重 W [hidden_dim, vocab_size]  
  
分割方式1 - 按行分割W：  
└─ W_part1 [hidden_dim, vocab_size/2] (设备0)  
└─ W_part2 [hidden_dim, vocab_size/2] (设备1)  
  
每个设备计算部分结果，然后通过AllGather拼接 

**✓ 优点**

  * 可扩展到单机显存无法容纳的超大模型
  * 保持批次大小（样本吞吐量高）
  * 理论上可扩展性强
  * 适合模型大小远超显存的场景

**✗ 缺点**

  * 通信开销大（激活值和梯度跨设备流动）
  * 实现复杂（需重写模型代码）
  * 集合通信频繁（AllReduce、AllGather）
  * 对网络拓扑敏感

3\. 流水线并行（Pipeline Parallelism）

定义与原理

将模型的不同层分割到不同设备，形成流水线处理多个样本。

**执行流程：**

时刻1：设备0处理样本1 (Layer 1-2)  
时刻2：设备0处理样本2, 设备1处理样本1 (Layer 3-4)  
时刻3：设备0处理样本3, 设备1处理样本2, 设备2处理样本1 (Layer 5-6)  
...  
称为"填满流水线"，实现硬件并行 

**✓ 优点**

  * 激活值通信量小（层间只传递激活值）
  * 相比张量并行通信开销低50%
  * 适合深层网络
  * 易于实现和调试

**✗ 缺点**

  * 流水线气泡（bubble）浪费计算
  * 需要多个样本同时处理（增加显存和批次约束）
  * 需要重新设计训练循环
  * 反向传播复杂度高

⚖️ 并行策略对比

维度 | 数据并行 | 张量并行 | 流水线并行  
---|---|---|---  
**通信量** | 梯度量（中等） | 激活+梯度（最大） | 激活值（最小）  
**内存占用** | 完整模型×N | 模型/N | 完整模型×N  
**实现难度** | 简单 | 复杂 | 中等  
**扩展性** | 8-64 GPU | 64+ GPU | 16-128 GPU  
**显存需求** | 无限制 | 受显存限制 | 需要多样本  
**最优场景** | 中小模型 | 超大模型 | 深层模型  
  
🔗 混合并行策略（3D Parallelism）

为什么需要混合并行？

单一策略无法达到理想的性能和可扩展性。现代超大规模LLM训练采用混合策略：

  * **数据并行** ：跨不同数据中心或机群
  * **张量并行** ：在高速互连的TPU Pod内
  * **流水线并行** ：跨多个GPU/TPU集群

3D 并行示意图

**资源分配：**

总设备数：256 TPU  
  
分割维度 1 - 数据并行（Data Parallel）  
└─ 分成8组，每组32个TPU处理不同样本  
  
分割维度 2 - 张量并行（Tensor Parallel）  
└─ 每组32个TPU进行张量分割  
└─ 分成4个子组，每个处理模型的1/4  
  
分割维度 3 - 流水线并行（Pipeline Parallel）  
└─ 每个子组的8个TPU处理8个流水线阶段  

#### 🔄 顺序组合方式

  * 先数据并行
  * 再张量并行
  * 最后流水线并行
  * 实现：每个维度形成一个"度"

#### 📊 通信特点

  * 数据并行：机间通信
  * 张量并行：Pod内通信
  * 流水线：层间依赖
  * 总通信 ≈ 三者之和

#### ⚙️ 配置选择

  * 模型大小决定张量并行度
  * 样本吞吐量决定数据并行度
  * 深度决定流水线并行度
  * 总度数 = DP × TP × PP

⚠️ 3D并行的挑战

  * **配置复杂** ：需要仔细选择三个维度的大小
  * **通信成为瓶颈** ：三种通信叠加，总量可能很大
  * **调试困难** ：问题可能来自任意维度
  * **负载均衡** ：需要平衡计算和通信

🏢 Google TPU/XLA 实现

TPU 硬件特性

TPU Pod 架构

代系 | 单芯片性能 | Pod规模 | 互连带宽 | 应用  
---|---|---|---|---  
**v4** | 275 TFLOPS | 128-512芯片 | 600GB/s (ICI) | PaLM 540B  
**v5e** | 197 TFLOPS | 8-256芯片 | 383GB/s | Gemini模型  
**v5p** | 459 TFLOPS | 8-256芯片 | 461GB/s | 超大规模训练  
  
集合通信操作（Collective Operations）

📡 核心集合通信操作详解

分布式训练中，设备间必须通过标准化的集合通信原语进行数据交换。下表列出四种最常见的操作：

操作名称 | 含义 | 典型用途 | 输入输出示意  
---|---|---|---  
**AllReduce** | 对所有设备数据执行归约（reduce，如 sum、max、mean），结果广播给所有设备 |  • 数据并行梯度同步  
• 参数更新前聚合  
• 全局损失计算  |  **示意：**  
设备0: [1, 2, 3]  
设备1: [4, 5, 6]  
设备2: [7, 8, 9]  
↓ AllReduce(sum) ↓  
设备0: [12, 15, 18]  
设备1: [12, 15, 18]  
设备2: [12, 15, 18]  
**通信量：** O(N)，其中N为张量大小   
**AllGather** | 收集所有设备的数据到每个设备上，无需计算 |  • 模型并行激活拼接  
• 张量并行权重聚合  
• 注意力层交叉设备  |  **示意：**  
设备0: [A0, B0, C0]  
设备1: [A1, B1, C1]  
设备2: [A2, B2, C2]  
↓ AllGather ↓  
设备0: [A0A1A2, B0B1B2, C0C1C2]  
设备1: [A0A1A2, B0B1B2, C0C1C2]  
设备2: [A0A1A2, B0B1B2, C0C1C2]  
**通信量：** O(N×P)，其中P为设备数   
**AllToAll** | 将每个设备的张量按切分维度分发到其他设备，实现数据重新布局 |  • 张量分割重新分布  
• 维度转置通信  
• 并行度变换  |  **示意（转置维度）：**  
设备0: [a, b, c]  
设备1: [d, e, f]  
↓ AllToAll(transpose) ↓  
设备0: [a, d]  
设备1: [b, e]（每个设备收到对应元素）  
**通信量：** O(N)，等价于一次轮转   
**ReduceScatter** | 先对所有设备数据做归约（如sum），再把结果按设备分片，每个设备只收到自己对应的部分 |  • 梯度并行优化  
• 减少通信量  
• 分布式求和分片  |  **示意（sum后分片）：**  
设备0: [1, 2, 3, 4]  
设备1: [5, 6, 7, 8]  
↓ ReduceScatter(sum) ↓  
设备0: [6, 8] (sum后的第1/2部分)  
设备1: [10, 12] (sum后的第2/2部分)  
**通信量：** O(N/P)，仅需N/P数据，每个设备只收一份   
  
⚠️ 集合通信的成本分析

**通信延迟模型（Latency Model）：**

T = α + β×N 

  * **α (startup latency)** ：启动开销，通常为微秒级（1-10 μs）
  * **β (per-byte time)** ：每字节传输时间，取决于带宽
  * **N** ：数据量（字节）

**例子（TPU v4, 600GB/s带宽）：**  
• AllReduce 1GB梯度：T ≈ 2μs + 1GB/(600GB/s) = 1.67ms  
• AllGather同样1GB：T ≈ 2μs + 1GB/(600GB/s) = 1.67ms  
• 但AllGather通信量是AllReduce的N倍（N为设备数） 

集合通信的优化策略

#### 🔄 Ring AllReduce

  * 相比Tree AllReduce更优
  * 充分利用带宽
  * 避免根节点瓶颈
  * 通信量：O(2N) vs O(N×log P)

#### 🔀 通信融合

  * 多个小AllReduce合并为一个大AllReduce
  * 减少启动开销α
  * 提高带宽利用率
  * 典型融合：多层梯度一起同步

#### ⚙️ 通信计算重叠

  * 计算后续层时，同时进行梯度通信
  * 减少总耗时
  * 需要异步执行支持
  * 吞吐量提升20-50%

#### 📊 ReduceScatter代替AllReduce

  * 当只需局部结果时使用
  * 通信量减少P倍
  * 结合AllGather实现功能等价
  * 梯度并行中常用

XLA SPMD 编译流程

从单机代码到分布式执行

# 第一步：用户编写单机代码 import jax import jax.numpy as jnp def transformer_layer(x, w): return jnp.dot(x, w) # 第二步：标注分割策略 mesh = jax.sharding.Mesh( devices=devices, axis_names=('batch', 'model') ) sharding = jax.sharding.NamedSharding( mesh=mesh, spec=jax.sharding.PartitionSpec('batch', 'model') ) # 第三步：XLA编译器自动变换 @jax.jit def compiled_fn(x): with jax.sharding_constraint(x, sharding): return transformer_layer(x, w) # 第四步：生成的代码包含： # - 张量分割和本地计算 # - AllGather/AllReduce 通信 # - 数据同步点 

实际案例：Gemini 模型训练

✓ Gemini 在 TPU v4 上的配置

参数 | 数值 | 说明  
---|---|---  
模型参数量 | 65B / 2T | 多个版本，最大2万亿参数  
TPU数量 | 512 | Google Cloud 超级集群  
数据并行度 | 2 | 跨2个TPU Pod  
张量并行度 | 8 | Pod内8向张量分割  
流水线并行度 | 32 | 32个流水线阶段  
批次大小 | 2048 | 总样本数/并行度  
吞吐量 | ~450K tokens/s | 全集群总吞吐  
  
💡 实践建议与最佳实践

选择并行策略的决策流程

**第一步：评估模型大小**

如果参数量 < 单机显存：  
└─ 只需数据并行，简单且高效  
  
如果 显存 < 参数量 < 10×显存：  
└─ 需要张量并行，TP度 = 参数量/显存  
  
如果 参数量 > 10×显存：  
└─ 必须3D混合并行，最大限度利用硬件 

**第二步：选择通信拓扑**

**TPU Pod 内部 (ICI高速互连，600GB/s)：**  
└─ 使用张量并行 + 流水线并行  
└─ 频繁的层间通信可以承受  
  
**TPU Pod 之间 (低速网络，~500Mbps)：**  
└─ 只用数据并行  
└─ 仅在一个step的末尾进行梯度同步  

性能优化技巧

#### 🎯 通信优化

  * 使用Ring AllReduce而非Tree
  * 融合多个小AllReduce为一个
  * 重叠通信与计算
  * 梯度累积减少通信频率

#### ⚙️ 内存优化

  * 激活值重计算（Checkpointing）
  * 梯度累积缓冲
  * 低精度训练（bfloat16）
  * 共享词嵌入权重

#### 📊 调度优化

  * 微批次大小需要平衡
  * 流水线气泡最小化
  * 负载均衡检查
  * profiling测量实际性能

⚠️ 常见陷阱

  * **通信瓶颈** ：高TP度时，集合通信成为主要开销（>50%执行时间）
  * **负载不均** ：不同设备计算时间不同，导致等待
  * **显存溢出** ：激活值缓存可能超过显存，需要checkpointing
  * **收敛变慢** ：某些并行配置会影响训练动态，需要调整学习率
  * **数值精度** ：多步AllReduce可能累积误差，使用mixed precision

📈 性能数据与基准

不同并行策略的扩展效率

配置 | 设备数 | 吞吐量(tokens/s) | 扩展效率 | 通信开销  
---|---|---|---|---  
单机（基准） | 1 TPU | 5K | 100% | 0%  
数据并行 (DP=8) | 8 TPU | 38K | 95% | 5%  
张量并行 (TP=8) | 8 TPU | 25K | 62% | 38%  
流水线并行 (PP=8) | 8 TPU | 32K | 80% | 20%  
3D混合 (DP=2, TP=2, PP=2) | 8 TPU | 35K | 87% | 13%  
  
✓ 大规模实验结果

  * **Gemini 2T参数模型** ：512 TPU v4上达到~70%扩展效率
  * **PaLM 540B模型** ：256 TPU v4上达到~75%扩展效率
  * **Grok-1模型** ：256 H100 GPU上3D并行，达到~65%扩展效率
  * **LLaMA 65B** ：64 A100 GPU上3D并行，达到~80%扩展效率

📌 核心总结

并行策略 | 核心特点 | 最佳应用  
---|---|---  
**数据并行 (DP)** | 样本分散，梯度同步 | 中小模型，高扩展性需求  
**张量并行 (TP)** | 权重分割，高频通信 | 超大模型，高速互连  
**流水线并行 (PP)** | 层分割，流水执行 | 深层网络，中等规模  
**3D混合** | 三种策略结合 | 万亿参数LLM训练  
  
关键决策因素

  * **模型大小** → 决定是否需要张量并行
  * **硬件拓扑** → 决定张量并行度和通信方式
  * **样本吞吐** → 决定数据并行度
  * **模型深度** → 决定是否有流水线并行
  * **通信带宽** → 决定集合通信频率

**LLM/TPU/XLA 并行策略完全指南**

涵盖数据并行、张量并行、流水线并行及其在Google TPU和XLA编译器上的实现

基于Google Gemini、PaLM等大规模模型训练经验，以及业界最佳实践

生成时间：2025年11月 | 最后更新于Google AI研究进展
