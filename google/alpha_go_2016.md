# AlphaGo（2016 年对战李世石版本） — 简明技术说明

聚焦原版 AlphaGo（2016 年李世石对局使用的系统）。本文档说明其核心模块、输入输出（以 `N=17` 的输入张量为准）、数据流与动作编码。

## 一、快速总览

AlphaGo 的决策由两类深度网络与蒙特卡洛树搜索（MCTS）协同完成：**策略网络（Policy Network）** 、**价值网络（Value Network）** 与**MCTS** 。系统每次接收当前棋盘状态作为输入，输出下一步的概率分布与最终选定落子。

## 二、输入（Input） — N = 17（17×19×19）

AlphaGo 使用 17 个平面（planes）构成输入张量：每层大小为 19×19。

通道编号| 含义| 说明  
---|---|---  
1–8| 当前执子方的最近 8 手| 二值平面：若该格在对应手被当前方下子则为 1  
9–16| 对方的最近 8 手| 二值平面：同上  
17| 当前落子方指示层| 如果当前为黑方则全 1，否则全 0（或反向表示），用于标识视角  
  
形状：`(17, 19, 19)`。空位由 0 表示。

## 三、策略网络（Policy Network）

**输入：**(17,19,19) 张量。  
**输出：** 长度为 `362` 的向量（361 个交叉点 + 1 个 PASS）。通过 softmax 得到先验概率 `P(s,a)`。

输出索引与棋盘坐标的映射：通常按行主序（row-major）将 19×19 展平并保留一个特殊索引表示 PASS。

## 四、价值网络（Value Network）

**输入：** 相同的 (17,19,19) 张量。  
**输出：** 一个标量 `v = V(s)`，范围在 `[-1, +1]`，代表当前执子方的胜率估计。

## 五、蒙特卡洛树搜索（MCTS）与数据流

MCTS 使用策略网络的先验作为拓展优先级（prior P(s,a)），并在拓展时调用价值网络获取评估 v。每次迭代包含选择 → 扩展 → 评估 → 回传。搜索若干次后，得到基于访问次数的最终分布 `π(a|s)=N(s,a)/ΣN`，以访问次数最多的动作作为系统输出。

### 输入 → 输出（简化视图）
    
    
    (17,19,19) --Policy--> P(s,a) (362)
                    \                 |
                     \--MCTS (调用 V)---/ 
                    --> π(a|s) (362) --> a*

### MCTS 节点统计

变量| 含义  
---|---  
N(s,a)| 访问次数（计数）  
Q(s,a)| 平均价值（历史回报）  
P(s,a)| 策略先验（来自 Policy Network）  
  
## 六、动作编码（Move Encoding）

动作空间包含 361 个交叉点与一个 PASS，总计 362 个动作。常见编码方式为：
    
    
    index = row * 19 + col   (0 <= index <= 360)
    index = 361              (PASS)
    行/列从 0 到 18 对应棋谱中的 A1..T19（跳过 I）或常见棋谱坐标。

示例（示意）：

动作（棋谱）| 含义  
---|---  
D4| 棋盘某一交叉点（示例概率：0.27）  
Q16| 另一个交叉点（示例概率：0.18）  
R6| （示例概率：0.12）  
PASS| 跳过一手（示例概率：0.01）  
  
## 七、训练与数据来源（简要）

原版 AlphaGo 使用两阶段训练：

  1. 监督学习：在人类职业棋谱上训练策略网络（模仿强棋手）
  2. 强化学习与自我对弈：使用策略梯度等方法通过自我对弈提升策略，并训练价值网络以预测最终胜负。

## 八、常见疑问（FAQ）

  * **为什么要用历史 8 步？** 历史步提供了局势演化信息（劫、临近手筋），帮助 CNN 捕捉时间上下文。
  * **为何有 PASS 动作？** 围棋规则允许玩家放弃一手，系统需将其视为合法动作。
  * **Policy 和 Value 为什么都输入相同张量？** 二者从同一局面出发，但解决问题不同：Policy 给出动作分布，Value 给出终局胜率。

—— 文档说明仅针对 AlphaGo 2016 年对战李世石的版本（N=17 输入格式）。如需导出为独立 HTML 文件、添加交互棋盘映射（坐标→索引表）或加入图示与 SVG，请回复说明你需要的格式和细节。
