<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>TPU 与 NVIDIA GPU（CUDA）对比与 Systolic Array 运行机制 — 研究报告</title>
  <style>
    body{font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial; line-height:1.6; color:#111; padding:24px; max-width:1000px; margin:auto}
    h1,h2,h3{color:#0b56a3}
    pre{background:#f6f8fa;border-radius:6px;padding:12px;overflow:auto}
    .card{border:1px solid #e6e9ee;padding:16px;border-radius:10px;margin:12px 0}
    table{border-collapse:collapse;width:100%;margin:12px 0}
    th,td{border:1px solid #ddd;padding:8px;text-align:left}
    .svg-wrap{display:flex;gap:12px;flex-wrap:wrap}
    .note{font-size:0.95em;color:#444;background:#fff8e6;padding:10px;border-left:4px solid #ffc107;border-radius:6px}
  </style>
</head>
<body>
  <h1>TPU 与 NVIDIA GPU（CUDA）对比与 Systolic Array 运行机制</h1>
  <p><strong>作者：</strong>（自动生成）&nbsp;&nbsp; <strong>目的：</strong>对比 TPU 与 GPU 的设计哲学、编译链与硬件执行机制，并通过 Systolic Array 的逐周期示例（2×2）解释脉动阵列如何完成矩阵乘法。</p>

  <h2>1. 概要</h2>
  <p>本报告总结如下要点：</p>
  <ul>
    <li>TPU 与 GPU 都通过大量并行硬件单元实现矩阵与标量运算加速，但设计哲学、可编程性与执行模型不同。</li>
    <li>TPU 编译链把高层算子逐步下沉（TensorFlow → XLA-HLO → LLVM IR → TPU HLO → TPU 指令），大量算子最终被合成为 MAC（multiply–accumulate）风格的阵列操作。</li>
    <li>Systolic Array 是一种时序（clocked）数据流阵列：每个 MAC 单元执行标量乘加，数据沿固定方向脉动流动，矩阵乘法由大量标量 MAC 的逐周期协作完成。</li>
    <li>"tile" 对应的就是脉动阵列（Systolic Array）的物理计算块大小，TPU 会把整个矩阵分成一块块 tile，每个 tile 在阵列上以时序脉冲方式流入、计算、流出。</li>
  </ul>

  <h2>2. Google TPU 与 NVIDIA GPU（CUDA）对比</h2>
  <div class="card">
    <table>
      <thead>
        <tr><th>对比维度</th><th>Google TPU</th><th>NVIDIA GPU (CUDA)</th></tr>
      </thead>
      <tbody>
        <tr><td>设计目标</td><td>专用深度学习矩阵运算加速（高吞吐、低内存带宽需求）</td><td>通用并行计算（图形渲染、AI、科学计算等）</td></tr>
        <tr><td>硬件单元</td><td>Systolic Array（大规模 MAC 阵列，如 128×128）</td><td>大量 CUDA 核心 + SM（调度/缓存/特殊单元）</td></tr>
        <tr><td>执行模型</td><td>静态编译调度、fused computation、流水化数据流</td><td>动态 kernel 调度、线程块/warp 并发执行</td></tr>
        <tr><td>数据流与内存</td><td>Tile 化、在片上 SRAM 重用、DMA 到 HBM</td><td>层级缓存（L1/L2/Shared/Global）、较频繁的全局内存访问</td></tr>
        <tr><td>可编程性</td><td>较窄但高效 —— 由 XLA 编译并下沉成矩阵指令</td><td>高度可编程（CUDA C++/PTX）</td></tr>
        <tr><td>适合的算子</td><td>矩阵乘法、卷积、transformer dot-product 等线性代数密集型</td><td>广泛（图形、物理、AI、数值计算）</td></tr>
      </tbody>
    </table>
  </div>

  <h2>3. TPU 编译链条（从高层到硬件）</h2>
  <div class="card">
    <table>
      <thead>
        <tr>
          <th>层级</th>
          <th>说明</th>
          <th>是否仍保留复杂操作</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Python + TensorFlow/Keras/JAX</strong></td>
          <td>用户使用 Python 编写高层神经网络模型，调用 TensorFlow、Keras 或 JAX API 定义计算图</td>
          <td>✅ 有复杂算子（卷积、激活、池化等）</td>
        </tr>
        <tr>
          <td><strong>XLA HLO</strong></td>
          <td>XLA 编译器将 Python 代码转换为标准化的中间表示 HLO，使用 HLO 指令集（基于 MLIR 标准）</td>
          <td>⚙️ 主要剩线性代数运算（矩阵乘法、加法、reshape、broadcast等）</td>
        </tr>
        <tr>
          <td><strong>LLVM IR</strong></td>
          <td>某些优化路径下转换为 LLVM 中间表示，支持标准 LLVM 优化 passes</td>
          <td>⚙️ 多为乘法、加法、访存等标量/向量算术指令</td>
        </tr>
        <tr>
          <td><strong>TPU HLO / TPU IR</strong></td>
          <td>专用的 TPU 指令集，针对 Systolic Array 硬件优化，支持 bfloat16、int8 等数据类型</td>
          <td>⚙️ 乘加为主，合并为大型矩阵运算块</td>
        </tr>
        <tr>
          <td><strong>TPU 微码 / 硬件指令流</strong></td>
          <td>最终转换为 TPU 芯片可执行的微码，控制 DMA、阵列调度和数据流</td>
          <td>✅ 几乎只剩 乘法+加法（MAC指令）</td>
        </tr>
      </tbody>
    </table>

    <h3>算子执行模式对比</h3>
    <table>
      <thead>
        <tr>
          <th>层级</th>
          <th>是否"一算子一执行"</th>
          <th>说明</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>TensorFlow算子层（TF Graph）</strong></td>
          <td>❌ 否</td>
          <td>会被融合、简化（如卷积+激活+加偏置）</td>
        </tr>
        <tr>
          <td><strong>XLA HLO 层</strong></td>
          <td>⚙️ 部分是</td>
          <td>HLO op 通常代表一个"逻辑算子"（如dot_general、add、relu）</td>
        </tr>
        <tr>
          <td><strong>TPU编译后（TPU Executable）</strong></td>
          <td>⚙️ 由编译器融合多个HLO为一个"computation"</td>
          <td>例如多个 add/relu/reshape 会融合进一次矩阵乘法执行块</td>
        </tr>
        <tr>
          <td><strong>硬件执行层</strong></td>
          <td>✅ 是"编译好的执行段"顺序执行</td>
          <td>每个段（program fragment）都是一个有序、预编排好的指令流</td>
        </tr>
      </tbody>
    </table>

    <p class="note">编程语言：主要使用 Python（TensorFlow 2.x, JAX）；标准：XLA HLO 基于 MLIR（Multi-Level IR）标准，支持 OpenXLA 生态。实际实现随 TPU 版本（v2/v3/v4/v5）略有差异，但编译流程保持一致。</p>
  </div>

  <h2>4. TPU 硬件执行机制（高层视角）</h2>
  <p>执行由三部分协同完成：</p>
  <ul>
    <li><strong>Host（或 runtime）</strong>：负责调度、加载执行单元、传递参数和触发 DMA。</li>
    <li><strong>On-chip memory / Buffer（SRAM）</strong>：存放 tile 数据、部分和（partial sums）、中间结果以减少对 HBM 的访问。</li>
    <li><strong>Systolic Array</strong>：时序阵列做实际的乘加运算。</li>
  </ul>
  <p>执行流程（简化）：Host 下发 fused computation → DMA 将 tile 拉入 on-chip SRAM → 控制器按固定时序将 tile 的 A 与 B 列/行注入阵列 → 阵列进行多拍累加 → 结果写回 SRAM/HBM → Host 发起下一个 fused computation。</p>

  <h2>5. Systolic Array（脉动阵列）运行机制要点</h2>
  <ul>
    <li><strong>每个 MAC 是标量运算单元</strong>：执行 P_out = P_in + A × B。</li>
    <li><strong>数据以时钟脉动流动</strong>：A 从左到右，B 从上到下，部分和沿固定路径累积。</li>
    <li><strong>时序展开（pipeline）</strong>：阵列需要若干时钟来“填充”（ramp-up）与“排空”（drain），但一旦 pipeline 满载，吞吐接近每拍输出。</li>
    <li><strong>Tile 化与数据重用</strong>：编译器按阵列大小对矩阵分块，尽量在片上复用 A/B 数据，降低带宽压力。</li>
  </ul>

  <h2>5.1 按时钟周期看数据流动</h2>
  <h3>矩阵相乘基础</h3>
  <p>矩阵相乘：</p>
  <p><strong>C = A × B</strong></p>
  <p>其中：</p>
  <p><strong>A = [a₁₁ a₁₂; a₂₁ a₂₂], B = [b₁₁ b₁₂; b₂₁ b₂₂]</strong></p>
  <p>目标是得到：</p>
  <p><strong>Cᵢⱼ = Σₖ Aᵢₖ × Bₖⱼ</strong></p>

  <table>
    <thead>
      <tr>
        <th>时钟周期</th>
        <th>MAC11 输入</th>
        <th>MAC21 输入</th>
        <th>MAC12 输入</th>
        <th>MAC22 输入</th>
        <th>说明</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>t=1</td>
        <td>A₁₁,B₁₁</td>
        <td></td>
        <td></td>
        <td></td>
        <td>阵列开始填充</td>
      </tr>
      <tr>
        <td>t=2</td>
        <td>A₁₂,B₂₁</td>
        <td>A₂₁,B₁₁</td>
        <td>A₁₁,B₁₂</td>
        <td></td>
        <td>第一行/列开始传播</td>
      </tr>
      <tr>
        <td>t=3</td>
        <td></td>
        <td>A₂₂,B₂₁</td>
        <td>A₁₂,B₂₂</td>
        <td>A₂₁,B₁₂</td>
        <td>全阵列激活，开始累加</td>
      </tr>
      <tr>
        <td>t=4</td>
        <td></td>
        <td></td>
        <td>A₂₂,B₂₂</td>
        <td>A₂₂,B₂₂</td>
        <td>结果逐步输出</td>
      </tr>
      <tr>
        <td>t=5</td>
        <td></td>
        <td></td>
        <td></td>
        <td>输出最后结果</td>
        <td>pipeline 排空</td>
      </tr>
    </tbody>
  </table>

  <h3>时序总结</h3>
  <p>填充阶段：需要 N + M - 1 = 3 个时钟周期（2+2−1），数据逐步注入阵列</p>
  <p>排空阶段：再需要约 2 拍（让最后的数据流完并截断）</p>
  <p>总时钟数 ≈ 5</p>
  <p>💡 一旦阵列"pipeline"充满，后续连续矩阵块就能以"每时钟输出一个结果"的高吞吐率运行。</p>

  <h2>6. 编译器优化与实际工程考虑</h2>
  <ul>
    <li><strong>算子融合（Fusion）</strong>：将连续的点乘/加/激活算子合并为单个 fused computation，减少调度与中间写回。</li>
    <li><strong>Tile 大小选择</strong>：受阵列尺寸、内存带宽、数据类型（bfloat16/int8/FP16）影响。</li>
    <li><strong>Buffer reuse</strong>：尽量在 SRAM 中复用数据，避免 HBM 往返造成带宽瓶颈。</li>
    <li><strong>精度与数值稳定性</strong>：低精度（bfloat16/int8）提升吞吐同时需要注意累加精度与缩放策略。</li>
  </ul>

  <h2>7. 结语与扩展阅读方向</h2>
  <p>TPU 的设计把硬件与编译器配合做到了极致：通过静态编译、tile 化和脉动阵列，把高层线性代数操作下沉为极高效的 MAC 流水线。GPU 则以灵活可编程和广泛适配性著称。两者各有侧重，常见的工程实践是根据目标负载选择最合适的硬件或混合使用。</p>

  <p style="margin-top:20px">如果你需要，我可以：</p>
  <ul>
    <li>将本 HTML 导出为可下载文件（或 PPT / PDF）。</li>
    <li>把示意图扩展成更细的每拍帧动画（逐帧 SVG 或 GIF）。</li>
    <li>把报告扩展为更学术的格式，添加参考文献和引用。</li>
  </ul>

  <footer style="margin-top:32px;font-size:0.9em;color:#666">自动生成 · 若需定制或补充实例（如 128×128 tile 调度示例或 XLA HLO 具体样例），请告知。</footer>
</body>
</html>